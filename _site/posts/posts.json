[
  {
    "path": "posts/an-okay-idea/",
    "title": "Project as an R package: An okay idea",
    "description": "The overarching problem I see with conforming analysis to the package domain, is that it introduces artifacts not of the project domain and that makes the project harder to comprehend",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2020-07-24",
    "categories": [
      "rstats",
      "workflow",
      "data science"
    ],
    "contents": "\n\n\n\nThis post is about an okay idea: structuring your R data analysis projects as packages.\nI say an “okay idea” because I’ve heard good points made by R community members I respect, and I agree with some of them - BUT ultimately I don’t think those points add up to a payoff that makes it a “good idea” for most teams in an industry context.\nThere are quite a number of resources that list the pros of this way of working, and I will briefly summarise those before I examine the cons. I’ll discuss:\ndouble handling of metadata\n\npointless artifacts\n\nmilesmcbain.xyz::added milesmcbain.yxz::noise\nduplication of metadata\nPros\nThe primary argument is usually that by structuring your project as an R package you gain access to a plethora of automations and facilities that are going to make your life better.\nYou get {devtools} and {usethis} for populating for speeding up menial development tasks including:\ndevtools::load_all() to populate your global environment with the most recent versions of your functions\nusethis::use_package() to declare a dependency.\nThe R CMD check routine can run on your project which will find things that do not comply with CRAN policy .e.g:\nundeclared dependencies\nincomplete function documentation\nYou get a well paved on-ramp to continuous integration services on GitHub to run automated test reports on your project code after pushing each commit.\nYou can wield the DESCRIPTION file, which:\ncan make your work more easily searchable\nclearly documents the authors/copyright holders and helps make your work citable (via citation())\ndocuments your dependencies\nIf you install your project package onto a system it automatically installs dependencies defined in the DESCRIPTION file.\nYou’re forced to put all your work in functions which hopefully triggers thought processes about composability and reuse.\nFurther reading\nThe first place I encountered the project as package idea was when I heard about Research Compendia and I believe it was through following rOpensci and Ben Marwick online.\nIt is notable that prominent proposals for the structure of a Research Compendium deviate significantly from that of a standard R package, for example: https://github.com/ropensci/rrrpkg. Ben Marwick has even created distinct devtools-like tooling for Research Compendia.\nDavid Neuzerling just wrote an excellent overview of the case for project as a package in his post Data Science Workflows.\nCons\nAnd now for the main event…\nDouble handling of metadata\nThe machine readable metadata in the DESCRIPTION file makes a lot of sense for something going to CRAN or some other deep storage archive. If you’re not actually releasing the project to public storage though some things become redundant.\nLet’s assume you use git for your project version control and host on GitHub. GitHub is your “deep storage”, and that means:\nThe Authors of the project are already tracked and displayed on GitHub\nTitle and Description are made redundant by your README\nURL and BugReports are useless because that’s the GitHub repo you work out of.\nVersion can be a git tag (GitHub release) 1\nImports, Depends, and Suggests describe your dependencies. Unfortunately they do not describe your dependencies in sufficient detail such that any guarantee can be made about your project package being able to be run after installation. Since your package isn’t on CRAN it is not being reverse dependency checked, and breaking changes in your dependencies will flow to you freely. And this can happen at any moment - Worked fine after install yesterday, broken today.2.\nTo be able to make a guarantee that your project will run with working dependencies at some point in the future you need to capture a known-good list of versions of all your dependencies3. To do this you use a lockfile or a dockerfile or both. Let’s say you use a lockfile created with {renv}. You now have a second place in your project repository that lists R package dependencies and version numbers.\nWhat’s worse is that these two statements of version dependence can easily become out of sync. This happens since lockfiles are typically generated by an automated snapshot process that examines your environment, while your DESCRIPTION is updated manually. Lockfiles obsolete dependencies in DESCRIPTION.\nPointless artifacts\nPointless artifacts are things that must exist for your project to be a valid package by CRAN’s policies but serve no purpose for a project that isn’t going to be publicly released on CRAN.\nThe most absurd pointless artifact for never-to-be released software is a license. You’ll need to declare an open source license acceptable to CRAN or R CMD check will throw an error in your continuous integration reports.\nAlso in this category are the paths you must use for your files. Documents that are to be built by your project go in ./vignettes. Report? It’s a vignette. Slide deck? That’s a vignette. Flexdashboard? That’s vignette too. What isn’t a vignette? Someone may well ask.\nAlternatively you could bury your documents in /inst/docs, since the inst folder is fair game, or you could introduce an .Rbuildignore file to indicate to R CMD check that a simple ./docs should not be cause for alarm. The .Rbuildignore file is the pointless artifact in this case.\nAdded noise\nEvery function call made in an R package that is not part of the base language needs to be tied to an appropriate namespace. The community has settled on the noisiest but most flexible approach to do this as standard, using the :: operator.\nI dug up a real snippet of code from a recent project to demonstrate the difference.\nHere’s the standard for project as a package:\n  plot_data <- daily_work %>%\n    dplyr::filter(IncidentTypeCategory %in% incident_type_categories) %>%\n    # determine ordering for category factor\n    dplyr::group_by(IncidentTypeCategory) %>%\n    dplyr::mutate(total_hours = sum(hours)) %>%\n    dplyr::ungroup() %>%\n    dplyr::mutate(\n      month = tsibble::yearmonth(floor_date(date, \"month\")),\n      category = forcats::fct_reorder(\n        IncidentTypeCategory,\n        total_hours,\n        .desc = TRUE\n      )\n    ) %>%\n    dplyr::group_by(month, category) %>%\n    dplyr::summarise(hours = sum(hours)) %>%\n    dplyr::ungroup() %>%\n    tsibble::as_tsibble(index = month, key = \"category\") %>%\n    tsibble::fill_gaps(hours = 0)\n\n  ggplot2::ggplot(plot_data, aes(x = month, y = hours, colour = category)) +\n    ggplot2::geom_path() +\n    ggplot2::scale_x_date() +\n    ggplot2::expand_limits(y = 0) +\n    ggplot2::facet_wrap(\n      ~category,\n      ncol = 1,\n      scales = \"free_y\"\n    ) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(\n      title = \"QFES monthly vehicle hours\",\n      y = \"vehicle hours\"\n    ) +\n    ggplot2::geom_smooth(se = FALSE, linetype = \"dashed\")\nand here’s what my team actually had:\n  plot_data <- daily_work %>%\n    filter(IncidentTypeCategory %in% incident_type_categories) %>%\n    # determine ordering for category factor\n    group_by(IncidentTypeCategory) %>%\n    mutate(total_hours = sum(hours)) %>%\n    ungroup() %>%\n    mutate(\n      month = yearmonth(floor_date(date, \"month\")),\n      category = fct_reorder(\n        IncidentTypeCategory,\n        total_hours,\n        .desc = TRUE\n      )\n    ) %>%\n    group_by(month, category) %>%\n    summarise(hours = sum(hours)) %>%\n    ungroup() %>%\n    as_tsibble(index = month, key = \"category\") %>%\n    tsibble::fill_gaps(hours = 0)\n\n  ggplot(plot_data, aes(x = month, y = hours, colour = category)) +\n    geom_path() +\n    scale_x_date() +\n    expand_limits(y = 0) +\n    facet_wrap(\n      ~category,\n      ncol = 1,\n      scales = \"free_y\"\n    ) +\n    theme_minimal() +\n    labs(\n      title = \"QFES monthly vehicle hours\",\n      y = \"vehicle hours\"\n    ) +\n    geom_smooth(se = FALSE, linetype = \"dashed\")\nNote how we had tsibble::fill_gaps even in the second example even though we didn’t need to. This is sometimes done as a kindness for the reader when using lesser known functions.\nI find reading from a %>% down into a namespace prefix particularly jarring. I can visually filter the prefixes if I concentrate and regain some of the nice natural language style dplyr flow, but I feel my eyes constantly being tugged to the left as I move down the lines. The procedure that reads a line of text starting from the left most character seems powerfully ingrained in my mind.4\nSurprisingly you must also use :: to prefix functions from core namespaces that are loaded automatically by R. So that means stats::, utils::, and tools:: etc with accompanying declarations in the DESCRIPTION file. A lot of commonly used functions are held in the core packages and I am regularly tripped up in package development trying to use functions without namespace prefixes that I thought were in the base language.\nAnother bit of noise gets introduced if you want to use global constants - that is data objects that are not functions. These are forbidden by default, but have valid use cases. For example I might define a global constant like EPSG_WEB_MERCATOR <- 3857 so later in my code I can do st_transform(sf_object, EPSG_WEB_MERCATOR) instead of having the magic number 3857 appear from the ether.5\nTo do this in a package project I must introduce this odd looking side-effect function into my code:\nglobalVariables(c(\"EPSG_WEB_MERCATOR\"), \"my_package\")\nWEBMERCATOR_EPSG <- 3857\nDissecting the killer features\nFrom what fans of the project as a package workflow have said to me, the two killer features are:\nHaving an easily installable artifact that will install its own dependencies\nEasy on-ramp to automated testing on GitHub continuous integration services.\nWith respect to 1. I’ve already debunked the dependencies aspect - making your project a package is not a robust form of dependency management6. On sharing, I’d argue git clone https://github.com/my_org/my_project is a pretty nice way to share a project, equivalent to install_github(\"my_org/my_project\").\nThinking about 2: Automated testing is great, and the package ecosystem has a nicely paved flow consisting of just a couple of {usethis} calls. But your automated tests are run within RMD check which adds overhead and a whole basket of compliance requirements that are pointless.\nI think people are missing the fact the that there is nothing spooky about the way {testthat} works. There is a function called testthat::test_dir that runs a folder of testthat tests in an environment that is setup by testthat.R . You could change a couple of lines in the GitHub action for CMD check, swapping rcmdcheck::rcmdcheck for testthat::test_dir and you have your automated testing without the tyranny of the CMD check and R package structure.\nWeighing the cons\nOkay so here’s where I go full subjective and get to the heart of why project as a package upsets me.\nAs data scientists we work in a field where there are very few objectively right outcomes. It doesn’t matter how we wrote our code, or how many unit tests it has, or even if our models converged7. What matters is that we have successfully constructed a convincing chain of inferential reasoning, lead by principled data analysis at every step, to a conclusion that is actionable somehow.\nAnd the key word is convincing. It’s not possible to write unit tests that validate the choice of one methodology over another. There are far too many subjective elements. So our code then has the key function of documenting our methodology, and our chain of inferential reasoning so that it can be audited, and validated by peers8.\nSo the whole shebang hangs, not on the functional correctness of our code, but on its clarity. And this is why I will fiercely advocate for writing code to be read. I try to structure my projects to be navigated, and a navigable project must be a reflection of its domain. Every choice I make prioritises surfacing the links my chain of reasoning over cpu cycles, over memory consumption, and everything else 9.\nThe overarching problem I see with conforming analysis to the package domain, is that it introduces artifacts not of the project domain and that makes the project harder to comprehend.\nPeople have said: “Yes but once you know how to make packages it actually makes things clearer”. I find this an inhumane response on a number of levels. It steepens the learning curve before collaborators can become contributors. This is cruel if there are less convoluted options that would suffice.\nMy response to advocates of project as a package is: ==You’re wasting precious time making the wrong packages.==\nInstead of shoehorning your work into the package development domain, with all the loss of fidelity that entails, why aren’t you packaging tools that create the smooth {devtools}/{usethis} style experience for your own domain?10\nNo really. Why. Aren’t. You. Doing. That?\nYou can cherry pick everything you like from package development and leave anything painful, annoying, or frustrating behind11. Your domain, done in your style, under your rules.\nConclusions, caveats, concessions\nMy aim here has been to provide some balance to a discussion that felt a bit one sided. I fully accept that I have but one vantage point and others will evaluate the tradeoffs project as package makes differently from theirs.\nFor my peers slugging away in industry data science roles, my argument essentially boils down to:\nThe two key benefits of the project as package workflow are prone to being overstated. For all the other things listed as ‘pros’, most can be realised under alternative workflows12. The loss of fidelity in terms of the mapping of the project to the domain is risky because it makes your methodology more difficult to verify.\nI am more convinced of the R package approach for reproducible research objects. Although they may not be going to CRAN, they fully expect to be archived. In this case the DESCRIPTION file has more value. It also makes sense to adhere to a really common standard, since if the domain shifts over a very long period of time, it may actually aid comprehension of the work to be in a more general standard form.\nWhere to next\nWhen I step back and look at what’s going on here I am reminded of this very lucid article shared by Hadley Wickham on Twitter recently. The article talks about the phenomenon of ‘bitrot’ and ‘bitcreep’ which are reciprocal software phenomena. In R, our collective tooling has seen significant bitcreep toward package development due to the amount and quality of developers who depend on these tools.\nConversely tools that break the package development paradigm for projects are succeptible to bitrot since they have fewer developers and everything that comes with that is its own self-reinforcing disincentive to users and future developers.\nThe way I see to combat this is with modularity. With small easy to maintain tools that we can compose to build our domain specific workflows. An example would be some kind of {testthis} package that encapsulates how to set up a project for automated testing with {testthat} sans CMD check. Another example might be a package that does something similar for linting.\nWith thanks to\nThe following #rstats community members who responded for my callout for feedback on Twitter, each opening my eyes to a different aspect of projects as packages:\nRaphaël Simon\nEdwin Thoen\nJon Harmon\nNick Tierney and Dean Machiori for being a sounding board for some of my agruments.\nMy team at QFES for being on board with putting clarity above all else, especially Anthony North.\n\nwhich you were probably doing anyway since it facilitates: devtools::install_github(\"milesmcbain/slippymath@0.3.1)↩︎\nYour own automated package checks may alert to the fact your package is already in a broken state if you are triggering them frequently enough↩︎\nAND your dependencies dependencies dependencies…↩︎\nSpeaking of %>%, you’ll have noticed we don’t have to do magrittr::%>% everywhere that is used. It is declared via importFrom(magrittr,\"%>%\") in the package’s NAMESPACE file. NAMESPACE is arguably pointless artifact if the project as a package is never installed.↩︎\nGlobal variables are dangerous. Global constants are useful communication tools.↩︎\nAnd if you don’t have proper dependency management, you are the dependency management!↩︎\nSince misspecified models can converge to an answer that is not well founded.↩︎\nWith possible peers including our future selves in 3-6 months time.↩︎\nAnd I’m lucky enough to work with some people that share this view.↩︎\nThere are many examples of R developers doing this. Some prominent examples to consider are: {workflowr}, {rrpkg}, {orderly}. I have made my own efforts public as discussed in my previous blog post. ↩︎\nTo me, the freedom to iterate toward for-purpose tooling is a core ingredient of a humane work environment. Over time the snags and rough-edges of ill-suited tools wear me down like water torture, until I get pangs of almost-physical pain each time I hit them.↩︎\nIf you like writing functions you owe it to yourself to give {drake} a look.↩︎\n",
    "preview": "posts/an-okay-idea/corrugated-cardboard-3853506_1280.jpg",
    "last_modified": "2020-10-09T14:12:04-04:00",
    "input_file": "project_as_an_r_package.utf8.md"
  },
  {
    "path": "posts/the-drake-post/",
    "title": "Benefits of a function-based diet (The {drake} post)",
    "description": "The {drake} post I've been threatening to make has landed. All 4000 words of it.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2020-04-30",
    "categories": [
      "rstats",
      "productivity",
      "workflow",
      "reproducibility",
      "debugging"
    ],
    "contents": "\n\n\n\n\nHeads up! This post assumes elementary knowledge of what {drake} does. If you’re at all hazy watch this excellent short video (5:48).\n\nA benefit of joining a fledgling Data Science team is that you get to shape the team culture and workflow. So when I joined QFES Futures just over a year ago I was keen to take the opportunity to try to steer us away from some of practices that I have seen make data science collaboration difficult in the past.\nOne of my main early catch cries was “reproducibility!”. But in this context I wanted that to mean more than just being able to blindly run eachother’s code and archive the same output. I want us to be able to reproduce the understanding of how the work is answering relevant questions. Apart from affording a degree of rigour, this has the benefit of reproducing skills across the team. Skills siloing leading to role “lock-in” is a path to collective unhappiness in my experience. 1\nSo now we get to the R package drake. Initially I was attracted to it as an easier alternative to make. Why would I want make? There were 2 main reasons:\nPretty much all our projects involve hitting enterprise databases with SQL queries to pull large volumes of data. This places two things in tension: For reproducibility’s sake you want to be running your data science pipeline end-to-end as often as possible. But for the sake of not being a jerk on a shared platform, you don’t want to be regularly fetching the exact same data you’ve received prior. I wanted caching, but with minimal explicit cache management.\nThe second thing I was craving was some common scaffolding we could use to structure and orchestrate our data science pipelines. Over the years I’ve tried a lot of the different R project templating tools out there. I’ve realised I don’t enjoy those that get too intricate with their folder structures. Inevitably you hit something that doesn’t neatly fit the schema.\nHappily I found drake had a lot to offer in both respects. And then some.\nOne type of question that emerges repeatedly in discussion with the drake-curious is something like “When does it become worth it to use drake?” or “I only do X type of work, will drake help me?”\nMy take is this:\nApproaches to data science pipelines using drake are valuable on projects large and small because they help you move faster by:\navoiding unnecessary computation time\neliminating common classes of bugs associated with interactive development\nproviding easy-to-use debugging access-panels\nfacilitating comprehensible project and code structure\nBUT. It’s not like you get all this for free just by using drake. You get nudges in the right direction. It is still up to you to establish a workflow that will fully convey these benefits. And I think this is where people are getting stuck - taking the first steps.\nSo with the rest of this post I’m trying to offer you an on-ramp by outlining my team’s current drake workflow. It is the result of many iterations over a year of #rstats data science project work and I hope it serves you well.\nFundamental Principles\nA problem my team has is that our workload can get choppy on short notice. Working for emergency services is by definition working through extreme and hence potentially unforeseen events. The media and politicians add their own spice to this mix lest thing get boring.\nComplexity does not survive in this environment. People need to be able to context switch between different analysis projects quickly, and workflow artifacts need to be zero-overhead to create, otherwise there simply ‘won’t be time’ to do the things that save us time.\nThe underlying principles of our workflow are a response to this environment:\nWe automate workflow tasks behind one line function calls. Very much inspired by {devtools} and {usethis}.\nWe use active countermeasures to ward off common bugs that can derail something due on short notice.\nWe try to write ‘self-documenting’ code in our drake plans, with an emphasis object/function names that articulate their role in the pipeline.\nI’ve put as much of this as possible in a package called {dflow} that helps create projects that follow our workflow.\nInitial Project Structure\nWe call dflow::use_dflow() to create starter projects that have this structure:\n.\n├── .env\n├── R\n│   └── plan.R\n├── _drake.R\n└── packages.R\n\nIt’s a simple flat structure that sets up the machinery for drake::r_make(). r_make() is a variant of make() that runs your pipeline described in plan.R within it’s own self contained R session. This is important because you want to avoid running your pipeline in an environment that contains things that have accumulated there during interactive development. Some of these objects might be stale - old function definitions for example - and this can lead to bugs or failure to reproduce results on a teammate’s computer.\nr_make() looks for a file called _drake.R which has the role of sourcing all your functions, making all the library() calls, and otherwise setting up any objects the pipeline needs to run. The final thing it does is return your plan and associated config to be run by the drake machinery.\nThe other files packages.R, and .env are about declaring dependencies and setting useful environment variables. They come populated with some stuff in them that I will return to later.\nMature Project Structure\nBelow is a more mature example that shows how things can evolve. Apart from R, doc is effectively the only standard one of these, since we use a function called dflow::use_rmd() that creates RMarkdown files in it. The other folders may not appear, or may appear under different names, depending on what the project is trying to do, and lead author’s taste.\n.\n├── .env\n├── .gitignore\n├── R\n│   ├── assign_crew.R\n│   ├── assign_stations_to_hexes.R\n│   ├── fetch_geoscience_data.R\n│   ├── get_block_data.R\n│   ├── get_crewnum_19.R\n│   ├── get_hexes.R\n│   ├── get_oms_responses.R\n│   ├── get_population_data.R\n│   ├── get_qld_sa1s.R\n│   ├── get_stations.R\n│   ├── intersect_population_hexes.R\n│   ├── knn_prediction.R\n│   ├── knn_prediction_future.R\n│   ├── plan.R\n│   ├── read_geoscience_data.R\n│   ├── summarise_hex_response_times.R\n│   ├── summarise_hex_structure_fires_future.R\n│   ├── summarise_hexes.R\n│   ├── tag_hexes.R\n│   └── tag_responses.R\n├── README.md\n├── _drake.R\n├── doc\n│   └── analysis.Rmd\n├── input\n│   └── qgso_sa1_pop.rds\n├── packages.R\n└── sql\n    ├── get_h3_remoteness_areas.sql\n    ├── get_hexes.sql\n    └── get_oms_responses.sql\nNotice how all the R files seem to be about doing one specific thing, and they’re all lumped in together under the R folder. This might feel weird if you’re used to a more traditional folder-based approach e.g. ./00_load/, ./01_wrangle/, ./02_model/. The difference is that the bulk of the project structure is provided by the plan.R file. To make that structure coherent it is useful to have functions named after specific steps. We have a convention one of these functions per file. This has advantages I will discuss very soon.\nPlan conventions\nHere is a slightly stripped back2 version of the plan.R file that corresponds to that project structure:\nthe_plan <-\n  drake_plan(\n    start_date = as_datetime(\"2018-01-01\"),\n    end_date = as_datetime(\"2020-01-01\") - seconds(1),\n\n    # spatial block is H3\n    h3_resolution = 6,\n    hexes = get_hexes(h3_resolution),\n\n    stations = get_stations(),\n\n    crews = get_crewnum_19(),\n\n    h3_geoscience_res6_source = fetch_geoscience_data(),\n\n    h3_geoscience_res6 = read_geoscience_data(h3_geoscience_res6_source),\n\n    qld_sa1s = get_qld_sa1s(),\n\n    future_pop = get_population_data(),\n\n    hex_populations = intersect_population_hexes(future_pop,\n                                                 qld_sa1s,\n                                                 hexes),\n\n    oms_responses = get_oms_responses(start_date, end_date),\n\n    tagged_oms_responses = tag_responses(oms_responses, hexes),\n\n    ## Structure Fires\n    hex_summary = summarise_hexes(\n      hexes,\n      h3_geoscience_res6,\n      tagged_oms_responses,\n      hex_populations\n    ),\n\n    hex_predictions = knn_prediction(hex_summary),\n\n    report = target(\n      command = {\n        rmarkdown::render(knitr_in(\"doc/analysis.Rmd\"))\n        file_out(\"doc/analysis.html\")\n      }\n    )\n  )\nIt’s all function calls. There is no explicit code for data wrangling, modelling, or plotting present in the plan. We try to keep the plan as close as possible to a high level description of what is happening. I wouldn’t expect you to fully follow what is going on, there’s a lot of assumed team context, but hopefully you can make out:\ndata acquisition steps with function names like get_*, fetch_*, read_*\ndata wrangling steps: intersect_population_hexes, tag_responses\na modelling step: knn_prediction\na report built from analysis.Rmd\nThe long and specific function names contextualise the processes they contain.\nWould you have guessed this project has had 3 people contributing code to it? That explains the slightly different choices of verbage in function names etc. Despite this it has maintained coherency.\nThe plan also induces structure that can help navigate and understand it. Let’s say we’re interested in what this hexes target is about and we want to see how it is defined. Our convention all but guarantees there will be a file, ./R/get_hexes.R, that corresponds to the get_hexes(h3_resolution) function call.\nWe could navigate to that file in our file browser. Or we could use the ‘jump to definition’ shortcut available in most IDEs/editors3.\nFunction Conventions\nOur convention is to match function argument names in the definition to the names of dependent plan targets where possible4. For example the definition of summarise_hexes looks like this:\n#' summarise_hexes\n#'\n#' @param hexes\n#' @param h3_geoscience_res6\n#' @param tagged_oms_responses\n#' @param hex_populations\n#'\n#' @return hex_summary\n#' @export\n#'\n#' @author Miles McBain\nsummarise_hexes <- function(hexes,\n                            h3_geoscience_res6,\n                            tagged_oms_responses,\n                            hex_populations) {\n...\n# convert all to tibbles and select columns\n# cascading left_join() calls\n...\n\n}\nwith all the argument names mirroring targets in the plan.\nThis convention speeds up the process of setting up suitable input values for interactive development of the function code, since we can read them directly out of drake’s cache! If you’re using RStudio, you have an addin provided by drake called “loadd target at cursor” which will load the target under the cursor into your global environment 5. This makes starting from a fresh session, jumping to some code, and getting set up to edit quite fast. Here’s a gif6:\n\n\n\nFigure 1: Jump to source, loadd targets at cursor, run code.\n\n\n\nFunctions are usually short, not much more than a screenfull of code. This maximises the effectiveness of the cache as a development accelerator. For example: if you combine a database query followed by some hectic wrangling into one target, you’ll be re-running the query every time you fine tune your wrangling. You’re better off splitting them into two functions so that the query only gets run once and cached.\nAutomated function templating\nWriting lots of these functions in individual files can get tedious, so I’ve automated this process. The {fnmate} package allows you to call functions into existence in a conventionally named file, just by writing a call to that hypothetical function:\n\n\n\nFigure 2: {fnmate} in use in RStudio\n\n\n\nBeware. This gets addictive and there’s no way back. I use it many times daily.\nRMarkdown Conventions\nWe’ll often have multiple outputs knitted from RMarkdown files. A report and a companion slide deck has been a favourite combo, or perhaps reports for peers vs reports for stakeholders.\nIn this scenario it made sense to promote objects like maps and plots that are shared across documents to plan targets. So it’s common to see stuff like this in our plans:\nthe_plan <-\n  drake_plan(\n    ...\n\n    coverage_map = create_coverage_map(isochrones, escad_incidents),\n\n    daily_work_series_plot = create_daily_work_series_plot(escad_incidents,   incident_categories)\n\n    ...\n\n    report = target(\n      command = {\n        rmarkdown::render(knitr_in(\"doc/analysis.Rmd\"))\n        file_out(\"doc/analysis.html\")\n      }\n    )\n\n  )\nWhere these targets are {mapdeck} and {ggplot} objects respectively. The RMarkdown then becomes extremely simple:\n...\n\n## 14 minute travel isochrone\n\n\\```{r, echo = FALSE, out.width = 100%}\nreadd(coverage_map)\n\\```\n\n...\nWhere drake::readd pulls coverage_map from the cache into the rendering environment. It also signifies to drake that report has a dependency on coverage_map.7\nAnd as a general consequence our RMarkdown documents have far less R code in them than I had become used to. Rendering completes in seconds and rarely errors.\nWhen we do include R code beyond reading targets from the cache, our principle is to keep it directly related to the presentation rather than computation of results.\nIt’s been interesting to see how niggles I have with Rmd and notebooks in general have evaporated under these conventions. Do you ever feel when looking at the source of an Rmd that the narrative of the text and the narrative of the code are out of sync somehow? This approach may release that tension for you.\nDebugging the pipeline\nWhen something goes badly wrong and errors, drake will give you the stack trace and the name the target on which the error occurred. From that you know which function to jump into so debugging from there becomes:\nload and inspect targets the function takes as inputs from the cache e.g. using loadd(target_name) or the ‘loadd target at cursor’ addin.\nPlay debug detective, e.g. :\nCheck the input conforms to your assumptions\njump to THAT target’s function if no\n\nInspect intermediate results\nExplain the problem to a duck\n\nAfter a fair bit of this I’ve come to appreciate how having these tightly focused functional units of pipeline logic that interface with eachother in regular ways helps you narrow in on a problem quickly.\ndrake provides some tools that can help with debugging but they’re not compatible with r_make(), since you need your REPL R session and the plan evaluation session to be one and the same. You can still use them though, just make sure you do so in a fresh R session. The workflow would be:\nsource(\"_drake.R\")\ndrake_debug(target = target_that_failed, plan = the_plan)\nAnd you’ll be immediately dropped into an interactive debugging session of the target function as if you had called debugOnce on it8. I rarely do this though, most of the time the first approach gets me there.\nActive Countermeasures\nHere’s where we return to the packages.R and .env:\npackages.R ships with:\nlibrary(conflicted)\nlibrary(dotenv)\nlibrary(drake)\nand here’s .env:\n_R_CHECK_LENGTH_1_LOGIC2_=verbose\n_R_CHECK_LENGTH_1_CONDITION_=true\nNow let’s talk about what’s driving this.\nAvoiding conflicts with {conflicted}\n{conflicted} makes sure you avoid the particularly insidious class of R bugs that arise from function masking in packages. That is: when two or more packages provide functions of the same name, the package you call library() on last has its function end up in your namespace. Change the order of your library calls, change the way your code functions. Yikes.9\nMy team tend to pull many different kinds of data together in our analyses. 20 or so library calls is in packages.R is not that uncommon. So there’s been plenty of scope for this issue to arise and hours have been lost. One of the things I despise about this bug is that the error messages you get are dumbfounding. Things you’re sure should work just suddenly don’t. It’s the kind of thing that makes you get all superstitious and fall back to turning it on and off again.\nconflicted will mean a few extra lines in the packages.R file of the form:\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nAnd yes doing that for filter every time you use it for the first time does get old. But it’s a tiny price to pay for never losing hours to this issue again.\nMaking R stricter with env vars\nThe only thing the {dotenv} package does is load the environment variables specified in the .env file. We prefer this to way to using an .Renviron file in the project root, since that would mask the user’s local .Revniron in their profile. .env is also a cross language standard for doing this.\nThe payload of the .env file are two R options that we’re turning on - this is what they do:\n_R_CHECK_LENGTH_1_LOGIC2=verbose makes the usage of ‘scalar’ logic operators like && and || with vectors longer than 1 element an error and includes tracing information. Why? Compare these expressions and their output:\nlibrary(tibble)\nlibrary(dplyr)\nmtcars <- as_tibble(mtcars)\nnrow(mtcars)\n\n# [1] 32\n\nmtcars %>%\nfilter(gear >= 4 || am == 1) %>%\nnrow()\n\n# [1] 32\n\nmtcars %>%\nfilter(gear >= 4 | am == 1) %>%\nnrow()\n\n# [1] 17\n\nSys.setenv(`_R_CHECK_LENGTH_1_LOGIC2_`=\"verbose\")\nmtcars %>%\nfilter(gear > 4 && am == 1)\n\n# ----------- FAILURE REPORT --------------\n# --- failure: length > 1 in coercion to logical ---\n# --- srcref ---\n# :\n# --- package (from environment) ---\n# package:dplyr\n# --- call from context ---\n# NULL\n# --- call from argument ---\n# gear > 4 && am == 1\n# --- R stacktrace ---\n# ...\nSee this issue for some exposition on what is happening.\nSo the variations read very similarly. They probably sound the same in your head, and yet the results! You may be confident you’ll never make this mistake. But how confident are you in spotting it in someone else’s code as a deadline looms?\n_R_CHECK_LENGTH_1_CONDITION_=true changes a warning that would normally be thrown by trying to use an if statement with length > 1 vector into an error. E.g.\nif(c(TRUE, FALSE)) print(\"Doin' stuff\")\n\n# [1] \"Doin' stuff\"\n# Warning message:\n# In if (c(TRUE, FALSE)) print(\"Doin' stuff\") :\n#   the condition has length > 1 and only the first element will be used\n\nSys.setenv(`_R_CHECK_LENGTH_1_CONDITION_`=\"true\")\nif(c(TRUE, FALSE)) print(\"Doin' stuff\")\n\n## Error in if (c(TRUE, FALSE)) print(\"Doin' stuff\") :\n##  the condition has length > 1\nThe rationale here is that upgrading the warning to an error forces us to fail early and hard. We don’t want to spot something like this after we’ve let the pipeline run for an hour, cache or no cache.\nAs a general principle, failing early and hard plays really nicely with drake since the processing up to the point of failure is not wasted. This idea can be extended from warnings to coding in assertions about input targets where appropriate.\nComplementary tools for reproducibility\nJust briefly I want to discuss managing versions of R package dependencies. We have two approaches that are employed depending on the durability of the project. Spoiler: neither of them involve Docker - we haven’t had the need yet.\nFor long lived projects\nFor things we want to be able to return to in months and years from now we use a kind of lighter-weight wrapper for RStudio’s {renv} called {capsule}. The idea behind capsule is that since we execute our pipeline in an isolated environment, it is that package environment and that environment only that needs to be clearly specified. A user’s interactive development environment does not need to be described and locked down in the same way.\nIn practice this side-steps a lot of awkwardness under renv that comes with trialling packages in development you quickly decide not to use, and accessing packages that are convenient for development but not required for the pipeline to run.10\nUsing capusle we can:\ncapture a description of the package environment set up by packages.R with capsule::create()\nrun a pipeline inside an environment that matches the description with capsule::run_callr(function() drake::r_make())\nFor short lived projects\nRecently we’ve evolved a different approach for things that are perhaps more time sensitive and at the same time more disposable. Think maps to respond to queries from ranking officials etc.\nWe use a packaged called {using} to map out any critical version dependencies in the packages.R. Instead of yelling to someone “This’ll only work with the dev version of mapdeck” as you email them a link to a gist or repo, you can write:\nusing(mapdeck, min_version = \"0.3.2003\", repo = \"https://github.com/SymbolixAU/mapdeck\")\nAnd as a nicety the user is prompted to install if the dependency is not satisfied.\nA driver for this is long compilation times for development versions of various packages that make building a complete environment using capsule a bit annoying for time sensitive things that will only be thrown away. 11\n“But will it scale?”\nWe’ve had as many as four data scientists contributing simultaneously to projects in this style. My sense from hanging out with you all online is that this is actually above average. One unexpected benefit of the one-function-per-file convention is that space is created for collaborators to make contributions without tripping over eachother.\nI can count the git merge conflicts we’ve had on one hand. They all happened in the plan file as that’s now the one place you can really step on someone. Conflicts in this file are not too bad to resolve since the code is so high level - just a list of targets and function calls.\nScaling to a large code base is not something I can comment on. We’ve probably done a few thousand lines of code at peak. I’ve heard that lots of code can challenge drake as it parses all it all to detect dependencies. This feels like a surmountable problem. Don’t let it hold you back.\nOn the other end of things I don’t think there actually is a project size too small. Many of the discussed benefits still apply and overhead is low thanks to automations. I’ve used this workflow a few times to create pipelines that output a single plot image. Even on a tiny project there’s something to be said for sticking to conventions to avoid bogging down in new decisions.\nConclusion\nIf you’ve had some programming education you may recognise a key philosophy of this approach as encapsulation. That’s the breaking of machinery into pieces, hiding it away behind descriptive labels, and clear interfaces. Encapsulation done well allows us to reason about a complex data science pipeline on different abstraction levels as necessary without getting overloaded.\nWhen you combine that encapsulation with a drake’s cache and dependency sniffing smarts, you get the ability to hone in on bugs extremely quickly. You also minimise the cost of having had them, since you reuse as much prior processing as possible in a principled way that guarantees reproducibility.\nUsing functions for the structure they can provide is different to the dominant way they are pitched in our community, as tools to avoid “repeating yourself”. In this context, I would change a popular maxim slightly to say:\n\nIf there’s a piece of code you’ve had to comprehend three or more times, consider writing a function (and choosing a good descriptive name).\n\nDoing this well still involves much art. Don’t worry if you’re not ready for the rest of the fucking owl yet. Building comfort with writing functions by forcing yourself to do it more often would be a great stepping stone to a workflow like this.\nThe end\nThanks for reading. My objective here hasn’t been to sell you on my tools and my workflow. Although they are there to be used and I welcome your feedback.\nI’m offering what I have for you to reflect on, adapt, and remix in the context of your own working styles, team dynamics, and coding preferences. I think workflows that work can’t be established any other way.\nWith thanks to\nNick Tierney for a thoughtful draft review of this post.\nMy colleagues, who have been on this journey with me for the last year, and among them especially Anthony North.\nrOpensci, and all drake’s contributors but especially Will Landau who is without a doubt the most responsive R package maintainer on the internet.\n\nI think I was first exposed to these kinds of ideas when I read this Scaling Knowledge at AirBnB blog post way back in 2016. Still worth a read now: https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091↩︎\nfull plan here↩︎\nF2 or Control + Mouse1 in RStudio↩︎\nThis may be not possible when using the more fancy dynamic target mechanisms, but we rarely use these.↩︎\nI’ve bound ‘loadd target at cursor’ to a keyboard shortcut here. It’s a simple matter to do this for VSCode or Emacs, ping me if you want an example.↩︎\nOkay some sleight of hand here. You’d need to have loaded the library calls first. I have a shortcut setup to do source(\"./packages.R\")↩︎\nYes. Incredibly drake parses the code inside your RMarkdown and registers them as dependencies of the plan.↩︎\nSince your target functions are just regular R functions you can already use debugOnce on them.↩︎\nAs people seem to enjoy pointing out, this functionality was added to base R since 3.6. Conflicted is still the superior choice in my opinion. It’s documentation is more coherent, and it let’s you take a lazy, function by function approach, addressing conflicts only if they actually arise in your code.↩︎\n{mapview}, {datapasta}, {styler}, {lintr}, {languageserver}, {fnmate} are all examples.↩︎\nOne could argue that in a time sensitive situation we need the protection of the capsule approach more than ever. I have a lot of sympathy for that argument.↩︎\n",
    "preview": "posts/the-drake-post/spinach_small.jpg",
    "last_modified": "2020-10-09T14:11:26-04:00",
    "input_file": "benefits_of_a_function_based_diet.utf8.md"
  },
  {
    "path": "posts/recover/",
    "title": "Stop() - breathe - recover()",
    "description": "Over the last couple of months a debugging technique involving an R function called `recover` has transformed my approach dealing with errors in R code. This post is an introduction to a ~20min video I've made where I demonstrate my `recover` approach on two examples.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2019-07-28",
    "categories": [
      "rstats",
      "debugging"
    ],
    "contents": "\nOver the last couple of months a debugging technique involving an R function called recover has transformed my approach dealing with errors in R code. This post is an introduction to a ~20min video I’ve made where I demonstrate my recover approach on two examples.\nI haven’t been this excited to share something with #rstats for a while - so trust me - this is a technique worth knowing. Especially if you subscribe to the tidyverse’s nested everything-in-a-tibble modelling workflow. When I’m manipulating list columns of tibbles and models I’ve found I’m that much farther from the context I need to understand errors. Getting that to context has previously been a labor intensive and demotivating task. Not so with recover! Just watch go the video! Come back here after for some post script comments.\n\n\nP.S.\nAs you saw, recover makes debugging complex errors easier by placing all the information you need to debug it at your finger tips. It. Is. Awesome! But it’s also just one tool in a full debugging arsenal. I certainly don’t have it enabled always. You’ll see pretty quickly how frustrating that is. Despite R’s reputation, most of the time an error message alone is enough for me to figure out what went wrong (with some help from Dr. Google).\nAt the end of the video I said you’ll need other techniques to tackle issues in your program logic that don’t result in an error. For those I highly recommend Kara Woo’s RStudio::conf 2019 talk: Box plots: a case study in debugging and perseverance where, among other things, she demonstrates nimble usage of debugonce. That’s another favourite debugging function of mine. 1\nThe code used in my examples can be found at: https://github.com/milesmcbain/recover_demo\nRelated Techniques\nRStudio has something like recover: the ‘re-run with debug’ link that appears alongside error messages in the console. And although this lets you browse the entire stack of code and variables, as far as I can see it doesn’t let you choose a frame and execute code within its context (like I did in my second example).\nBreakpoints are something that I have moved away from over time, despite having used them heavily with other programming languages prior to using R. I am not fully sure why this is, although I think they’re just generally less useful when you have a strong REPL like R.\nA Thankyou Owed\nI think a strong debugger is often a draw card that drives people to IDEs, so having these tools available within any R session opens up the playing field for different R editing setups. It really is amazing to have a tool like recover available within any R REPL. Hats off and thankyou to the R-Core team and contributors!\n\nUsing a lot of debugonce has made me realise how important program structure and style is to having debuggable code. I don’t think you can go past Jenny Bryan’s UseR 2018 talk: Code Smells and Feels for an approachable introduction to ideas along those lines.↩︎\n",
    "preview": "posts/recover/recover_vid.png",
    "last_modified": "2020-10-09T14:11:52-04:00",
    "input_file": "stop_breathe_recover.utf8.md",
    "preview_width": 893,
    "preview_height": 503
  },
  {
    "path": "posts/the-roots-of-quotation/",
    "title": "The Roots of Quotation",
    "description": "In this post I use Lisp as a prop to have the quoting conversation from the start. It's a deep dive that will hopefully arrive at a deep understanding.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-07-26",
    "categories": [
      "rstats",
      "lisp"
    ],
    "contents": "\n\n\n\nRecently I’ve been trying to learn more about Non-standard evaluation and specifically quoting in R. It’s been a pretty frustrating time. The best way I can describe it is a constant maddening feeling like I am coming in halfway through a conversation.\nOne thing I kept noticing again and again is R documentation that references quoting often references Lisp, as if deferring explicit definition of some concepts to your knowledge of Lisp. For example, ?bquote1 :\n\nAn analogue of the LISP backquote macro. bquote quotes its argument except that terms wrapped in .() are evaluated in the specified where environment.\n\nYou do know Lisp right? Yeah nah, me either. But I have been taking some baby steps in that direction with Peter Seibel’s excellent (free online) book Practical Common Lisp.\nIn this post I’m going to use Lisp as a prop to have the quoting conversation from the start. From there I’ll build up to ‘quasiquotation’ and ‘unquoting’, and finally distill the jargon into some short take-aways. It’s not an overview. It’s a deep dive that will hopefully arrive at a deep understanding. Let me know if you found the read worth it.\nIn the beginning there were seven functions\nIt seems that when people talk about the birth of Lisp, they frequently cite a paper by John McCarthy called Recursive Functions of Symbolic Expressions and Their Computation by Machine Part 1. I found the human-readable tribute version, The Roots of Lisp by Paul Graham, to be a bit more digestible.\nAs told by Graham, In McCathy’s original paper in 1960, he laid out 7 primitive operators and then proved those 7 could be combined into a recursive evaluation system for arbitrary expressions of a list-based form. That system became Lisp, but more importantly for this discussion, one of those 7 infinity stones, that have shaped modern computation ever after, was called quote.\nDiscovering this was extremely exciting to me. In quote we have something analogous to the Higgs Boson of Non-Standard and Tidy Evaluation! Genealogy, etymology, and heritage - all subsequent quoting functions and evaluation systems flow from quote. More exciting still is that quote can be described and understood in quite simple terms, owing to the simplicity and elegance of McCarthy’s evaluation system.\nA Study of Lisp’s Quote\nAs a primer, here’s a great quote about quote:\n\nQuote may seem a bit of a foreign concept, because few other languages have anything like it. It’s closely tied to one of the most distinctive features of Lisp: code and data are made out of the same data structures, and the quote operator is the way we distinguish between them. Paul Graham\n\nNow I’m going to present a scenario that will give you some insight into the importance of quote in making evaluation work, but before I can do that I’ll need to give you a brief primer of Lisp functions and evaluation rules.\nLisp in a paragraph\nIn Lisp everything is either an atom or a list. atoms are literals like 3.14, \"Hello\", or names of variables or functions like: myvar, list, quote. Lists are denoted with () and contain atoms and or other lists. When writing Lisp programs you write lists, each of which is interpreted as a function call unless otherwise instructed2. In a function call the first list element is a name identifying the function and the following elements become its arguments. To make that concrete, this R expression:\nlist(1, 2, list(3, 4) )\n\n#> [[1]]\n# [1] 1\n#\n# [[2]]\n# [1] 2\n#\n# [[3]]\n# [[3]][[1]]\n# [1] 3\n#\n# [[3]][[2]]\n# [1] 4\nhas this lisp equivalent:\n(list 1 2 (list 3 4))\n\n; => (1 2 (3 4))\nThe set up\nLet me now introduce you to a Lisp function called set. It has a role as an assignment function. Given that, you might try something like:\n(set myvar \"myvalue\")\nBut that’ll get you an error: The variable MYVAR is unbound. So the program tried to evaluate myvar and found it was not defined - we knew that - we were just trying to define it!\nSo okay let’s go out on a limb:\n(set \"myvar\" \"myvalue\")\nError again: The value \"myvar\" is not of type SYMBOL when binding SYMBOL\nYikes. So it won’t take a string as a variable name because that’s not a ‘symbol’, but if we give it plain text it will try to evaluate it and find it unbound. You may have guessed where this is going. We need to tell the program: Don’t evaluate myvar, it’s not code, it’s data for the set operation. We need to use quote:\n(set (quote myvar) \"myvalue\")\nThere are several shorthands available to make this less cumbersome. In Lisp (quote myvar) can also be written as 'myvar, and furthermore, these are all equivalent:\n(set (quote myvar) \"myvalue\")\n(set 'myvar \"myvalue\")\n(setq myvar \"myvalue\")\nIt may not surprise you to learn setq (a quoting set) is a standard way to perform assignment in Lisp. In fact it’s verging on impossible to do something resembling traditional assignment in Lisp without a quoting function being involved somehow.3\nGetting us on a technicality\nAn interesting discussion point arises from the previous example: quote seems to return the text we supply as a ‘symbol’ since that is the input expected by set. How can we reconcile that with descriptions of quote that say it ‘simply’ returns its argument unevaluated? Consider these R and Lisp expressions:\n## R\nx <- quote(y)\nclass(x)\n# [1] \"name\"\n\n;; Lisp\n(setq x (quote y))\n(type-of x)\n; => SYMBOL\nCome on R, the thing I passed you didn’t have a class. You definitely evaluated something, and Lisp you’re no better… can unevaluated text have a data type?\nIt turns out that these type of definitions of quote are correct only if we apply a narrow and technical definition of evaluate. The relevant background to comprehend this definition is well articulated by Peter Seibel in Chapter 4 of Practical Common Lisp. Here’s a crude summary:\nCompilers and interpreters are most often opaque to programmers. It is usual though that they are made up of subprograms. There is usually a distinct program with the responsibility of parsing the text we type and creating a structure of machine-readable of tokens or symbols. Then there is usually a distinct program responsible for mapping parts of this structure to low level code to be executed on the processor. This last program - the one in control of execution - is considered to be the ‘evaluator’ of the code.\nSo what this means is that code can be transformed and structured by sub-programs4 without technically being evaluated until it is processed by the ‘evaluator’. ==In the Lisp family of programming languages it is possible to instruct the evaluator to leave off, and instead of an evaluation result, the machine-readable symbolic structure that would have been ‘evaluated’ is returned. This is what quote does.==\nCode as Data\nIf you have investigated much of the tidy eval documentation you may know the machine-readable symbolic structure returned by quote as an Abstract Syntax Tree. In Lisp it’s just regular nested lists. In the previous section, the R class \"name\" and Lisp type SYMBOL are atomic elements of these structures respectively. Quoting more complex code will yield different structures. Since Lisp uses plain lists for data and code, it is intuitive that syntactical structures can be manipulated using regular list mechanisms - somewhat surprisingly this holds for R as well:\n## R\ny <- quote(abs(x - 1))\nclass(y)\n# [1] \"call\"\n\ny[1]\n# abs()\n\ny[2]\n#(x - 1)()\n\ny[[1]] <- quote(sin)\ny\n# sin(x - 1)\n\n;; Lisp\n(setq y (quote (abs (- x 1))))\n(type-of y)\n; => CONS\n\n(car y)\n; => ABS\n\n(cdr y)\n; => ((- X 1))\n\n(setf (car y) (quote sin))\n\n(print y)\n; => (sin (- x 1))\nLisp notes: car can be thought of as y[[1]] and cdr can be thought of as y[-1]. setf is a general version of setq that can handle the equivalent of [<- in R.\nData to Code\nThe last piece of the puzzle is taking the data that we have manipulated and evaluating it as code once again. Then we’re metaprogramming. Continuing from above, we do this in both Lisp and R with eval, after setting a value for x.\n## R\nx <- 1\neval(y)\n# [1] 0\n\n;; Lisp\n(setq x 1)\n(eval y)\n; => 0.0\nQuote derivatives\nWith our understanding of quote firming up it is natural to see if we can use it to understand other related concepts like unquoting and quasiquotation. Applying the natural interpretation of the prefixes ‘un’ and ‘quasi’ we might guess: * unquoting is the reverse of quoting? Machine-readable back to human readable? Can it be be ‘evaluation’? We already have a word for that. * quasiquotation is something that is almost but not quite quotation?\nAnd we’d be wrong on two counts. But it’s not our fault - these are terms are straight up jargon. The concepts they describe are simple to explain and motivate though. A last Lisp example should do the trick:\nLet’s say we are operating in a scenario where we are trying to compose a list to eventually become function call that takes a function as an argument. We have to build two lists, quoting most of the arguments, except the last one where we want the result of the code in that position, (+ 1 1), as the argument.\nWe are actually quoting more often than we are evaluating, e.g:\n(list 'myfun 'arg1 (list 'myfun2 'arg2 (+ 1 1)))\n; => (MYFUN ARG1 (MYFUN2 ARG2 2))\nUgh, very noisy. It would be convenient syntactically if we could switch into a complimentary mode where instead of telling Lisp what we want to quote, we just tell it what needs to be evaluated. Quoting becomes the standard procedure.\n==This anti-quoting mode is what was named ‘quasiquotation’==. In Lisp you switch it on using the ‘`’ operator, and you signal things needing evaluation using a ‘,’. So when ‘quasiquoting’ the above function would be:\n`(myfun arg1 (myfun2 arg2 ,(+ 1 1)))\n; => (MYFUN ARG1 (MYFUN2 ARG2 2))\nSignaling things need evaluating in the midst of quasiquotation is what has been called unquoting. Notice the effect of unquoting is that the result of the evaluation, '2', becomes data in the list that is returned.\nIn simple cases like above unquote is effectively saying ‘evaluate’. But it does make sense to have a different term, since it is possible to have nested quasiquotation and unquote is not equivalent to evaluate in that context - it’s an evaluation somewhere down the line.\nFYI you’ve now seen the Lisp ‘backquote macro’ referred to by R’s bquote that I showed the help for in the introduction.\nA Distillation\nNow let me try and distill the concepts we have covered into a few memorable one-liners: * quote transforms code into data. * That data being the machine-readable symbolic structure needed to formally evaluate it. * quasiquotation is a mode that assumes we want all code transformed to data. * unquote flags code that we wish to have the result of transformed to data. * In the context of quasiquotation.\nConclusion\nThe big light bulb arising from this for me was when I gained appreciation for quote as a code -> data transformation. I think the main reason I found studying quoting in Lisp revelatory in this regard is that the mechanics are a) simple and b) predictable. Code and data are both plain lists. These properties make Lisp a useful sandbox for building confidence with metraprogramming ideas.\nIn R you can really only have a) OR b) but not both. Base R quoting and Non-Standard evaluation functions are few and seem simple on the surface but are dogged by behaviour that changes with context (e.g. substitute) or are less general/robust than we might like (e.g. bquote).\nTidy eval deserves our appreciation for doing something similar to what purrr did for functional programming: it’s rounding off the jagged edges in the API and making metaprogramming in R much more stable and predictable. The trouble, as I have discussed previously, is this has come at the cost of simplicity. There’s much work to be done to break that down and this post has just focused on one foundational aspect.\n\nExamples: bquote (base R), quasiquotation (rlang), quasiquotation (Advanced R)↩︎\nIn truth there are other types of calls, and the ones Lisp nuts really bang on about are macro calls↩︎\nThought experiment for the reader: Does this hold for R?↩︎\nCalled names like ‘lexical analyser’ and ‘parser’↩︎\n",
    "preview": "posts/the-roots-of-quotation/Quote---2-.jpg",
    "last_modified": "2020-10-09T14:11:35-04:00",
    "input_file": "the_roots_of_quotation.utf8.md"
  },
  {
    "path": "posts/solving-the-challenge-of-tidyeval/",
    "title": "Solving the Challenge of Tidyeval",
    "description": "Ok here's the vignette... scroll scroll scroll... now I just quo... no wait enquo, no wait sym? Ahhh! equos that has to be it...",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-05-23",
    "categories": [
      "rstats",
      "tidyverse",
      "tidyeval"
    ],
    "contents": "\n\n\n\n\nThis dplyr code sure is working great! Now to wrap it up in a nice function to keep things clean and modular… oh fuck! The arguments aren’t interpreted correctly… that’s right. That tidyeval business. Let’s google ‘programming with dplyr’ - Ok here’s the vignette… scroll scroll scroll… now I just quo… no wait enquo, no wait sym? Ahhh! equos that has to be it… shit shit shit! Why is this not working?! Okay okay, I’ll read it top to bottom AGAIN. ‘Quote’? ‘Closure’? ‘Quosure’? ARGH I don’t care, tell me how to MAKE THIS WORK!\n– Me, Every 3 months.\n\nIt’s well known that the R community at large finds tidyeval challenging. To get a feel for the sentiment just check out the tidyeval tag on the RStudio Community Forum. In this post I discuss why tidyeval is challenging, and what we might be able do about it.\nNot your average tidyverse package\nAn overarching theme in R’s tidyverse is reducing the user’s cognitive load by creating functional APIs that align closely to the tasks the user needs to perform. The convention is to name functions using carefully chosen natural language verbs that allow the user to intuitively discover the function they need to manipulate their data using code completion mechanisms. There is no expectation that the user should read all the documentation up front before trying to use the API - indeed it’s common to see tidyverts tweeting or blogging joyously about new tidyverse functions they ‘just discovered’. 1\nrlang - the package that powers tidyeval - is a very different animal. It does not conform to these conventions. This pretty much sums up the problem:\nlibrary(purrr)\nlibrary(magrittr)\nlibrary(rlang)\n\nrlang_namespace <- ls(\"package:rlang\")\nlength(rlang_namespace)\n# [1] 429\n\nkeep(rlang_namespace, ~nchar(.x) <= 7)\n#  [1] \":=\"      \"!!\"      \"!!!\"     \"%@%\"     \"%|%\"     \"%||%\"    \"abort\"  \n#  [8] \"are_na\"  \"as_box\"  \"as_env\"  \"as_list\" \"bytes\"   \"call_fn\" \"call2\"  \n# [15] \"chr\"     \"chr_len\" \"cnd\"     \"cpl\"     \"cpl_len\" \"dbl\"     \"dbl_len\"\n# [22] \"dots_n\"  \"enexpr\"  \"enexprs\" \"enquo\"   \"enquos\"  \"ensym\"   \"ensyms\" \n# [29] \"env\"     \"env_get\" \"env_has\" \"exiting\" \"expr\"    \"exprs\"   \"f_env\"  \n# [36] \"f_env<-\" \"f_label\" \"f_lhs\"   \"f_lhs<-\" \"f_name\"  \"f_rhs\"   \"f_rhs<-\"\n# [43] \"f_text\"  \"flatten\" \"fn_body\" \"fn_env\"  \"fn_fmls\" \"get_env\" \"inform\" \n# [50] \"inplace\" \"int\"     \"int_len\" \"invoke\"  \"is_box\"  \"is_call\" \"is_env\" \n# [57] \"is_expr\" \"is_lang\" \"is_list\" \"is_na\"   \"is_node\" \"is_null\" \"is_raw\" \n# [64] \"is_true\" \"lang\"    \"lang_fn\" \"lgl\"     \"lgl_len\" \"list2\"   \"ll\"     \n# [71] \"locally\" \"modify\"  \"na_chr\"  \"na_cpl\"  \"na_dbl\"  \"na_int\"  \"na_lgl\" \n# [78] \"names2\"  \"new_box\" \"new_cnd\" \"new_raw\" \"node\"    \"ns_env\"  \"pkg_env\"\n# [85] \"prepend\" \"qq_show\" \"quo\"     \"quos\"    \"raw_len\" \"seq2\"    \"set_env\"\n# [92] \"splice\"  \"squash\"  \"string\"  \"sym\"     \"syms\"    \"type_of\" \"unbox\"  \n# [99] \"UQ\"      \"UQE\"     \"UQS\"     \"warn\" \nThere are a whopping 429 objects/functions in the rlang namespace! Around quarter of which favour this terse, abbreviated style of function naming. Now a good chunk of the functions do use classic tidyverse verb_object style, however if you read through the Programming With dplyr vignette you’ll find all the tidyeval functions presented, bar one, come from that list above.\nI suspect the reason for this minimalism is that the user has to nest rlang functions and operators within their classic dplyr code to use them. %>% can’t be used to help keep things readable. So rlang seems to have been designed to fade into the background as much as possible.\nThis results in a user experience that is quite apart from your average tidyverse package. The user almost certainly needs to consult the documentation upfront to identify the abbreviation that needs to be invoked in this huge namespace.\nNot for your average tidyverse user\nThe problem is it’s not as simple as go to the documentation -> find the function -> get the job done. A lot of space in ‘Programming with dplyr’ and other communications on the topic is dedicated to trying to explain the concepts of metaprogramming. This raises the question of who is tidyeval really for?\nDoes your average data scientist looking to make their code more readable or accessible by building functions, really need to grok metaprogramming concepts like quoting, clousures, quosures, environments, abstract syntax trees, and overscoping? These terms refer to computer science literature, and specifically the LISP family of languages. They have historical significance, but I would argue they are not particularly great names for the concepts they embody. ‘quote’ especially, because it clashes with the process of writing a literal character vector, i.e. 'this' - something most tidyverse users do every day.\nThe choice to frame tidyeval in computer science terms presents a big problem: it means to onboard your average tidyverse-using data scientist, you have to teach them computer science. This supposes they have both the time and desire to be taught computer science/metaprogramming when all they really wanted to do was put their code in a function! I think this is the source of a lot of the bewilderment and frustration with the approach taken in Programming With dplyr: Users just don’t expect to need a computer science lecture to get this type of task done.2\nMaking tidyeval more tidyverse\nLet’s keep in mind: It is design decisions taken in dplyr and ggplot2 that have driven the need for tidyeval3. It is not at all given that a data scientist needs to take on metaprogramming concepts to write a function that encapsulates manipulating or plotting tidy data.\nSo the question I am considering is: Can we bend this powerful and feature rich metaprogramming framework provided by rlang in such a way, so that an API more empathic toward the majority of tidyverse users emerges? Is there a flavour of tidyeval that can feel like it was designed for me, like the rest of the tidyverse?\nMy hypothesis is that this should be possible to do, using rlang as a platform, without writing much code at all! I am not here to tell you I have solved it, but I have made a start on a more tidyverse tidyeval.\nI’ve written a package called friendlyeval that reduces the namespace of rlang down to 5 functions and 3 operators. The Programming With dplyr vignette has been distilled down to about a quarter of the length, and rewritten using task-focused language that tries to reference the data science context. You won’t find any mention of quosures, environments, quoting, or abstract syntax trees etc.\nThe ‘killer’ feature is that friendlyeval code can be automatically converted to rlang code using an included RStudio addin. Importantly this means: 1. You don’t have to have production code depend on something not maintained RStudio developers. 2. Your friends don’t have to know you took the easy route.\nHelp Wanted\nI’m really keen for collaborators on this ‘compatibility layer’ idea. Naming things is hard! And that’s pretty much the central challenge of this whole thing. I haven’t even considered ggplot2 and I feel the chance of me getting it right on my own is pretty low. Feedback is going to give this idea legs, so please throw ideas at me over in the GitHub issues. Thanks for reading!\nHeader Image credit: By Robson#, CC BY 2.0, https://www.flickr.com/photos/robson/9972796593/in/photostream/\n\nJust searching ‘#tidyverse discovered’ on Twitter is a thing to behold.↩︎\nAs it happens, my bachelor’s degree was in computer science and I still find tidyeval bewildering.↩︎\nAlthough I think it might have been the problems with naming plot elements in ggvis that really started us down this path.↩︎\n",
    "preview": "posts/solving-the-challenge-of-tidyeval/9972796593_14695448e7_b.jpg",
    "last_modified": "2020-10-09T14:11:44-04:00",
    "input_file": "solving-the-challenge-of-tidyeval.utf8.md"
  },
  {
    "path": "posts/ropensci-onboarding2/",
    "title": "Where is the value in package peer review? Reflection #2 on rOpenSci Onboarding",
    "description": "How is a package peer reviewer’s time best spent? When is the best time in a software package’s life cycle to undertake peer review? A user-driven perspective.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-26",
    "categories": [
      "rstats",
      "rOpenSci",
      "open science"
    ],
    "contents": "\n\n\n\nIf you read my reflection #1 on rOpenSci Onboarding, then you know I see value in the Onboarding process. A LOT of value even. This post is about where that value lies.\nThis question has important corollaries which I will explore here based on my experience as a reviewer of bowerbird:\nHow is a package peer reviewer’s time best spent?\nWhen is the best time in a software package’s life cycle to undertake peer review?\nDoing a good job\nAs I’ve read the growing number of reflective posts by other Onboarding reviewers, I’m struck similarities in our experiences. Mara Averick, Verena Haunschmid, and Charles Gray all place emphasis on reviewing from a “user’s” perspective. For Mara and Charles, this perspective was used to gain traction after initial nervousness about their ability to do a good job.\nI shared these initial butterflies. bowerbird’s code base was quite large on the first submission. The code embedded a lot of domain specific API knowledge. It was disorientating and I felt like I was thrashing around stumbling onto issues without undertaking a systematic survey.\nProbably recalling Mara’s experience, I eventually honed in on the idea that to do a good job, all I had to do was identify things that would hold bowerbird back from being a widely-used and popular package. And the best way to identify these was from the “user’s” perspective.\nThat perspective implied the systematic process that I was seeking. So I started from the typical user entry points - README, vignettes, examples in help files. I went through them and tried to use the functions as suggested to download bulk repository data (bowerbird’s usecase). I had success! I uncovered quite a few idiosyncrasies in its API and documentation that created a bumpy ride for a new user. The authors were very good about acknowledging and fixing these.\nI did review code as well. But it was a natural part of exploring the behaviour as I followed the documented workflow thread. And If I think about the impact the more technical issues I found had on the package as a whole? It’s probably negligible. Sure you can argue the code is a bit more robust, a bit easier to maintain. But it would be hard to justify maintaining code that isn’t seeing any use!\nI wasn’t lucky enough to see Onboarding editor Maëlle Salmon’s recent hatted SatRday talk, but I had to smile when I spotted this slide in her deck:\n\n\n\nFigure 1: Yes indeed!\n\n\n\nIt seems we agree on the priorities. Maëlle gives technical considerations the lowest value.\nThe R community has repeatedly heard Hadley Wickham argue that reducing cognitive load is more important than cpu load in most analysis scenarios. Following that school of thinking, in a talk called “If You Build it They Won’t Come”, Hadley seems to be pushing package developers to focus on making their packages marketable. The user is very much at the forefront of his development ideology.\nIt is interesting that Onboarding reviewers like Mara, Charles, and myself all initially assumed a more technical code-driven approach was where the value in peer review came from. Should we be surprised such value arose from the user-driven approach? A user-driven review school seems a logical counterpart to the user-driven development school that has gained popularity in the R community.\nTo a software developer, detailed user feedback is an extremely rare and valuable commodity. It’s quite amazing to receive it from multiple sources for free. The realisation that this is where the bulk of the value lies has important implications for the scaling of package peer review processes: Veteran UseRs are in much more plentiful supply than veteran package developers.\nTiming it right\nArmed with my user-driven heuristic, it didn’t take me long pick up on some of editor Noam Ross initial queries and determine bowerbird needed to be split into two packages. The documentation and API struggled to paint separate coherent workflows for two very different classes of user.\nFollowing the split I think the authors began to feel some concern at the scale of the issues emerging in review. They worried aloud they were wasting our time and sought to engage reviewers and editors in a conversation about whether bowerbird had been submitted too early in its life cycle. Opinions were split on whether early or late stage review is better. I was very much in favour of early and did not see an issue with bowerbird being submitted as it was.\nI would argue that under the user-driven school, early has to be better. There is more opportunity to use feedback to inform the package API and workflow. Or stated another way: the value obtained from the peer review is maximised. The maximisation comes at the cost of editor/reviewer time, but so long as expectations and timelines are set appropriately I don’t see this as an obstacle.\nLeaving review until late stage allows for the possibility that work done in the name of open source and open science might be wasted to some degree. I see that as tragedy worth avoiding at significant cost.\nConclusion\nA reviewer’s time is best spent on the things most impactful on a software package’s success. This is an uncontroversial statement. However, the user-driven school of package development & review suggests the things that most impact package success are those that affect the end user - documentation, API, and workflow. It is therefore in improving these things that the bulk of the value in package peer review lies.\nFalling out of this discussion are two important take aways from the user-driven school of package peer review:\nAs a useR, you are qualified as a potential package reviewer if you have some appreciation for what makes your favourite packages useful.\nAs a package author, engage early with a package peer review process if you see value in it, to get the most out of it. This is BEFORE things are ‘perfect’.\nEpilogue\nbowerbird recently cleared the review process and obtained Peer Reviewed status. In my biased opinion, the main vignette is one of the best I have read, and a fine example product of the user-driven school I have discussed here.\nCongratulations to the authors Ben Raymond and Michael Sumner!\nHeader Image: A Satin Bowerbird, credit: benjamint444, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=1588854\n\n\n",
    "preview": "posts/ropensci-onboarding2/1200px-Satin_bowerbird.jpg",
    "last_modified": "2020-09-16T07:07:02-04:00",
    "input_file": {}
  },
  {
    "path": "posts/a-fully-dockerised-ghost-blog/",
    "title": "A Fully Dockerised HTTPS Ghost Blog",
    "description": "The installation documentation for Ghost is comprehensive, but beyond my skill as a system administrator to complete. Docker to the rescue!",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-08",
    "categories": [
      "ghost",
      "docker"
    ],
    "contents": "\n\nNote: This blog is no longer built with Ghost, I’ve gone static with Distill. These instructions should still serve you well.\n\nGhost is the blogging platform that powers this blog. You can pay for a hosted instance on https://ghost.org/, but as open source software, you can also self-host. The installation documentation for Ghost is comprehensive, but beyond my skill as a system administrator to complete. Luckily Docker allows hackers like me (and you!) to piggy back off the system admin skills of those far more knowledgeable.\nThis is the post I wish I had found when I spent a day hunting for the magic Docker image that would get me up and running with a self-hosted Ghost instance.\nPrerequisites\nThis is the stuff you need to have and be somewhat familiar with to self host Ghost the way I will describe.\nA domain for your blog. I use DNSimple to purchase and manage mine.\nA Virtual Private Cloud (VPC) provider. I recommend an instance with at least 2GB RAM, 1 VCPU, with a public static IP address. I use Vultr.\nComfort with administering a VPC instance over an ssh connection: Create folders, edit a file, run a couple of commands.\nSome conceptual familiarity with Docker. We’ll use very simple docker-compose commands. So you’ll need docker-compose and docker.io installed on your VPC instance.\nDocker Composition\nUsing the docker-compose framework we build a network/cluster of container hosts that together form a complete solution.\nThe composition has these parts: 1. Ghost, our blog platform. 2. Optionally RStudio Server. My blog relates to R, this comes in really handy for me. You can remove it if you want to. 3. An nginx reverse proxy server that automatically configures itself to manage ssl handshakes, upgrade http to https, and route connections to hosts we specify in the composition. 4. An SSL certificate service that automatically obtains ssl certificates for https from Let’s Encrypt for hosts we specify in the composition. These are automatically renewed.\ndocker-compose.yml\nThis is the configuration file that defines the composition above. I’m going to give you the whole thing here and then commentate the various sections in the rest of this post.\n\nversion: '2'\n\nservices:\n\n  ghost:\n    image: ghost:1.21.3-alpine\n    restart: always\n    environment:\n      NODE_ENV: production\n      url: https://milesmcbain.xyz\n      mail__transport: SMTP\n      mail__options__service: Mailgun\n      mail__options__auth__user: <My Mailgun User>\n      mail__options__auth__pass: <My Mailgun Password>\n      VIRTUAL_HOST: milesmcbain.xyz\n      LETSENCRYPT_HOST: milesmcbain.xyz\n      LETSENCRYPT_EMAIL: <My Email>\n    volumes:\n      - ~/data/ghost:/var/lib/ghost/content\n\n  rstudio:\n    image: rocker/tidyverse\n    restart: always\n    ports:\n      - \"8787:8787\"\n    environment:\n      USER: <My RStudio User>\n      PASSWORD: <My RStudio Password>\n      ROOT: \"TRUE\"\n      VIRTUAL_HOST: <My RStudio Domain>\n      LETSENCRYPT_HOST: <My RStudio Domain>\n      LETSENCRYPT_EMAIL: <My Email>\n    volumes:\n      - ~/repos:/home/milesmcbain/repos\n\n  nginx-proxy:\n    image: jwilder/nginx-proxy\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - \"/etc/nginx/vhost.d\"\n      - \"/usr/share/nginx/html\"\n      - \"/var/run/docker.sock:/tmp/docker.sock:ro\"\n      - \"/etc/nginx/certs\"\n\n  letsencrypt-nginx-proxy-companion:\n    image: jrcs/letsencrypt-nginx-proxy-companion\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock:ro\"\n    volumes_from:\n      - \"nginx-proxy\nghost:\nThe image: version tag can be updated to the latest version on the docker repository.\nNODE_ENV: ‘production’ is strongly recommended by Ghost developers for a live blog.\nGhost configuration options listed in the Installation and Setup Guide are configured as environment: variables. Instead of JSON style nesting ,{}, nested options are delimited using __.\nThe combination of VIRTUAL_HOST, and LETSENCRYPT_HOST instructs the nginx-proxy and letsencrypt-nginx-proxy-companion to redirect http traffic for http://milesmcbain.xyz to https://milesmcbain.xyz and then on to the Ghost instance. This is important because configuring a https url: ensures Ghost will only accept https connections. I recommend this because administering a Ghost blog involves logging into to a web interface.\nSSL certificates required for https are automatically obtained and renewed from Let’s Encrypt using the LETSENCRYPT_HOST and LETSENCRYPT_EMAIL as parameters. That email address will be emailed if the certificate for the host is due to expire. I have not seen one of these yet, since the automation is working.\nmail__ configuration is optional. Setup with Mailgun is fairly painless, It’s described in the Ghost Installation and Setup Guide. There are two main mail usecases: Emailing subscribers about new posts (if you use a subscribe button) and allowing blog contributors to recover from forgotten passwords. If you’re a solo blogger like me and not using email subscriptions you probably don’t need it.\nvolumes: is important. It defines folders on your VPC host that are mapped to the instance. You absolutely need a folder on your VPC host to keep your blog content. Otherwise it will not persist when the Docker container is stopped and restarted. E.g. for VPC provider maintenance or a Ghost version upgrade. ~/data/ghost:/var/lib/ghost/content maps the folder /miles/data/ghost on my VPC file system to /var/lib/ghost/content in the Ghost container. miles is my user I created to administer the blog on the VPC. The data/ghost is in my home directory.\nrstudio:\nThe config in ports: maps port 8787 on my VPC to port 8787 in the rocker/tidyverse Docker container where RStudio is listening. Remember to open port 8787 on your VPC using your VPC management console.\nUSER, PASSWORD and ROOT relate to the an RStudio user that can log into the server. ROOT adds the user to sudoers - this is useful if you want to install additional R packages and their Linux dependencies.\nI have set up this container with it’s own subdomain that I configured with my DNS provider. Let’s say it is rstudio.milesmcbain.xyz - so http traffic to that will be forwarded to https and then to the RStudio login page, so that login can be done securely.\nIn volumes: an area of my VPC file system is mapped to the rocker/tidyverse container so my RStudio project repositories will persist.\nnginx-proxy: and letsencrypt-nginx-proxy-companion:\nThe configuration here is the boilerplate for a minimum viable setup. The mapped volumes: are especially critical and should not be tinkered with. Both of these containers support additional configuration options that are discussed on their Github repo README pages linked above.\nPossibly of interest is SSL certificate key size and auto-renewal interval configured in letsencrypt-nginx-proxy-companion. The defaults are sensible though.\nStarting your Ghost blog\nSo if you’re still with me, you must be getting pretty keen to fire up your new blog! To do so:\nIn an ssh session to your VPC host: Copy or fork and clone the docker-compose.yml file into a folder in your VPC host user’s home directory, e.g. /<your user>/compose/docker-compose.yml\nGo to the folder and make necessary edits to the file (e.g. using vim or nano), replacing my domains, <My Email>, <My RStudio User> etc. with your personal config.\nSave and exit docker-compose.yml and run the command docker-compose up, from the same folder as docker-compose.yml. You may need to prefix with sudo.\nSit back and enjoy the show in your terminal. For a little while Docker will be downloading image data, after which it will start the composition and you will get a very detailed log showing what each container in the composition is doing:\n\n\n\nFigure 1: A test I ran on my laptop\n\n\n\nAfter the log has settled down, verify your blog is working by navigating to https://<your domain>. You should see a stock Ghost theme with example posts and text.\nBack in your terminal session, Hit ctrl + c to interrupt the Docker containers, this will trigger Docker shutdown. Follow with the command: docker-compose down to remove temporary data.\nRun: docker-compose up -d, to start your blog again, this time as a background process. You will get some simple messages about containers starting, and not the detailed log. Now you can close the terminal, exit ssh etc, and your blog will remain live.\nNavigate to https://<Your Domain>/ghost to set up your admin user and log in to the blogging platform.\nHave fun tweaking your blog, downloading themes, and writing posts!\nNotes\nIn step 4, keep an eye out for issues in the log, particularly with ssl certificate registration. You need to have a domain that can be resolved via DNS to be issued with a certificate.\nAdditional Reading\nThings not already linked that I found useful along the way to figuring this out:\nDigital Ocean Documentation on setting up a VPC server. Useful for any VPC provider. In particular: How to Set Up SSH Keys\nDeploy Ghost blog using Docker and Caddy\nsimple-lets-encrypt-docker-compose-sample\nIf you have any suggested improvements, need a hand, or just want to show off your blog to me, feel free to @ me on Twitter or leave a comment below.\n\n\n",
    "preview": "posts/a-fully-dockerised-ghost-blog/g1.png",
    "last_modified": "2020-09-16T07:07:02-04:00",
    "input_file": {},
    "preview_width": 953,
    "preview_height": 560
  },
  {
    "path": "posts/ropensci-onboarding1/",
    "title": "Waterslides vs Ivory Towers: Reflection #1 on rOpenSci Onboarding",
    "description": "Onboarding is a waterslide. rOpenSci have created a rewarding and fun process that generates high quality R packages.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-06",
    "categories": [
      "rstats",
      "open science",
      "rOpenSci"
    ],
    "contents": "\n\n\n\nLast week editor Maëlle Salmon closed issue #139 on ropensci/onboarding and thus marked the end of 7 months of preening the Australian Antarctic Division’s bowerbird. I take a sense of pride as a reviewer of this package. It was unquestionably improved by the onboarding process, and it was just a really great feeling to be a part of a process that: A. worked so well, and B. was a worthwhile use of volunteered time.\nWhen I say ‘7 months’ it makes the process sound grueling. It was not. I estimate I spent around 20 hours over that time. Far from feeling worn down, all I’m thinking right now is how much I want do it again!\nWhich brings me to the first of a couple of reflections I plan to write on the Onboarding process.\nOnboarding is a Waterslide\nAs a reviewer, the Onboarding process feels like this: 1. Initial excitement and trepidation when the editor invites you to review -> standing at the top 3. A moment of blind panic when you first lay eyes on all that code -> taking the plunge 4. A receding sense of helplessness replaced by determination and enjoyment as your contribution grows -> surfing the tube 5. Joy as the final ticks are given -> splashdown 6. Happiness, relief, satisfaction as the package goes live -> coming out in the wash 7. A desire to do it again -> running to beat the queue.\nMaybe that analogy is a bit cute. But don’t let that make you miss the point: rOpenSci have created a rewarding and fun process that generates high quality R packages. It’s just like the unconference. It’s addictive. It’s a positive feedback loop that improves both the packages and the people involved.\nThe issue pulse speaks for itself. They have a steady stream of packages and reviewers knocking their door down. If some people tire of the fun, others arrive keen to take their place. The system is mostly run on public volunteer labour, so they are inclusive, and have scalability. Future availability of editors is the main bottleneck, but there are capable candidates either waiting in the wings or being nurtured in the rOpenSci community right now.\nI think some people have the impression that onboarding is about this: \nIt is. But it’s called Onboarding for a reason. Onboarded packages get added to a bona fide R package repository (like CRAN or Bioconductor) maintained by rOpenSci. For me this is the really exciting thing - it’s totally punk.\n\n\n\nFigure 1: rOpenSci founders celebrate another package sucessfully onboarded\n\n\n\nOverlooking the Ivory Tower\n\n\n\nFigure 2: An unknown R developer awaits news of their CRAN submission\n\n\n\nThis rOpenSci repository is almost the perfect antithesis of what R has in CRAN. Where the process for getting added to rOpenSci is completely transparent, CRAN is notoriously opaque, inconsistent, and aloof. I could present you with evidence that CRAN’s centralised Ivory Tower model is cracking under the current load, but I’ll save that for a dedicated post. The other contrast I’ll make is in package quality.\nPick 5 random packages from CRAN’s task views. How many have at least one vignette? How many have a URL for a repository? How many have more than the bare minimum autogen manual mechanically describing functions? I reckon you’ll get one or two best - and these are the hand picked cream of the crop. It’s not that CRAN has slipped - it’s that the bar has been lifted. RStudio, rOpenSci, and other new wave empathetic developers have seen to that.\nThe rOpenSci model can never do the volume CRAN can with automation, but it is crushing it in terms of quality. All onboarded packages have a vignette, a repo, and made sense to at least three people from diverse backgrounds. Anyone can look up the public Onboarding review to get additional insight into why a package works the way it does. An rOpenSci package is not just ‘guaranteed to work’ - it’s guaranteed to be workable. And that is something pretty darn valuable indeed.\nKeen to be a part of it? Go here to regsiter as an rOpenSci reviewer.\nHeader image credit: By Koala:Bear - https://www.flickr.com/photos/ohfuckkit/199410084/, CC BY-SA 2.0, https://commons.wikimedia.org/w/index.php?curid=11078509\n\n\n",
    "preview": "posts/ropensci-onboarding1/640px-Wet-n-Wild_Water_World_Australia_Mach_5.jpg",
    "last_modified": "2020-09-16T07:07:02-04:00",
    "input_file": {}
  },
  {
    "path": "posts/alt-r-with-vscode/",
    "title": "Going Alt-RStats with VSCode",
    "description": "I never want to see another CSV that isn't rainbow!",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-02-22",
    "categories": [],
    "contents": "\nMid 2017 I got hooked on VSCode. The extension ecosystem is nothing short of a candy shop for editor nerds. I’ve gone from using VSCode for WebVR development - Can you say Chrome debugger?! - to recently wondering if I could build an #rstats data science stack from all these juicy extensions.\nIt turns out I was wondering at the perfect time. Yuki Ueda is improving the VSCode R extension weekly, and Randy Lai has just taken the whole thing to the next level with an implementation of an R language server - More about that in a minute. Both of these are under heavy active development.\nA RRogues Gallery\nHere I’ll walk you through the pieces of my current setup. Most of these are VSCode extensions, some are other bits of software.\nR Extension for VSCode\nThere’s a good set of features here already. The main niceties I enjoy are highlighters for Roxygen and .Rmd. You also have help with managing your R console. Linting is included, if that’s your cup of tea.\nR LSP Client\nThis is a key piece of the setup, but doesn’t do a lot on it’s own. It’s the VSCode client for the R language server, see next item.\nR Language Server\nThis is where the fun starts. It’s actually an R package! A server written in R that implements the language server protocol (LSP) for R. The LSP is designed to be editor agnostic, so this can work with VSCode alternatives like Atom, Sublime etc.\nAt time of writing this thing is still in early dev, but it already has some amazing tricks. Some of this stuff is beating out RStudio in UX.\nHow about hover help?\n\n\n\nFigure 1: Scrollable help file on hover\n\n\n\nCompletions and parameter hints?\n\n\n\nFigure 2: Autocompletion and function parameter hints\n\n\n\nStyle hints?\n\n\n\nFigure 3: lintr style hints\n\n\n\nrtichoke (Formerly Rice)\nrtichoke is an alternative R console cut from the same cloth as iPython. In fact it is written in Python, but if you can get past that you’ll find it has some really sweet features. Syntax highlighting with a variety of colour options, multi-line editing, vim/emacs keyboard shortcuts, and an oh so addictive enter-bash mode as seen here.\nMy favourite thing is the fast and robust completion mechanism. It just feels tighter than RStudio. It doesn’t glitch out. Try it and see what I mean. This goes some way towards compensating for the VSCode editor’s completions being unaware of your R session - something I thought I could not live without until I found rtichoke. \nBe sure to check out the options listed in the README. I’ve found the following useful to add to my .Rprofile:\n\noptions(\n    rtichoke.auto_indentation = FALSE, \n    # Makes alignment looks natural when sent to rtichoke by VSCode\n    rtichoke.complete_while_typing = FALSE\n    # Stops a brief stutter compiling a large parameter completion list for S3 methods like `plot()`\n)\nRainbow Brackets\nI remember first seeing this feature back in MS Excel in ’97, when I thought I was ever so clever hacking my calculus assignment. I now suck at calc, so the joke’s on me. But the joy of this not entirely nostalgia driven. I feel like I am genuinely less anxious reading heavily nested code in rainbow flavour.\n\n\n\nFigure 4: Lookin’ sharp!\n\n\n\nRainbow CSV\nAt the risk of overdoing it… I never want to see another CSV that isn’t rainbow! Note the column identifier on hover!\n\n\n\nMaterial Icon Theme\nR logos, Travis icons, colour codes etc for your file tree. They look smart and improve scanability.\nVim Extension\nMuch more than your average Vim emulator. It integrates with Neovim and even emulates a few popular plugins. I find it close to the real deal without the typical Vim config overhead. Makes RStudio’s Vim emulation look lame by comparison.\nMaking up the Difference\nI have a repo of misc functions, some of which fill in things I miss from RStudio. For example: rend() is my answer to the ‘Knit’ button. I may spin these out into a package of their own in the future.\nConclusion\nI am optimistic about VSCode’s future #rstats prospects given how quickly the language server has come along. There’s also some keenness building to try rolling my own VSCode extension or contributing those above. One thing I miss from RStudio are tidyverse-aware completions, e.g. completing the column names of a piped dataset in dplyr functions. Seems like a good opportunity to make a contribution.\nThus completes my post on my VSCode setup. If you’re loving VSCode as well, feel free to @ me on Twitter with your favourite extensions!\n\n\n",
    "preview": "posts/alt-r-with-vscode/hover_help2.gif",
    "last_modified": "2020-09-16T07:07:02-04:00",
    "input_file": {}
  }
]
